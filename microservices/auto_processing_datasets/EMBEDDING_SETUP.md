# Configuration du Mod√®le d'Embedding Nomic-Embed-Text

Ce guide explique comment configurer et utiliser le mod√®le `nomic-embed-text:v1.5` d'Ollama pour enrichir l'analyse des textes avant le traitement par le LLM principal.

## üìã Pr√©requis

1. **Ollama install√© et d√©marr√©** sur votre machine
2. **Mod√®le nomic-embed-text t√©l√©charg√©** :
   ```bash
   ollama pull nomic-embed-text:v1.5
   ```

## üîß Configuration

### 1. Ajouter le mod√®le d'embedding dans les param√®tres

1. Acc√©dez √† la section **Settings > AI Settings** dans l'interface
2. Cliquez sur **"Add Local Model"** ou **"Add External Model"**
3. Configurez le mod√®le comme suit :

   - **Model Name**: `nomic-embed-text:v1.5`
   - **Base URL**: `http://localhost:11434` (ou l'URL de votre instance Ollama)
   - **API Key**: (laissez vide pour Ollama local)
   - **Retry Requests**: `3`
   - **Paginate Rows Limit**: `500` (ou selon vos besoins)

### 2. V√©rifier que le mod√®le est d√©tect√©

Le syst√®me d√©tecte automatiquement les mod√®les d'embedding en cherchant :
- Le mot "embed" dans le nom du mod√®le
- Le mot "nomic-embed" dans le nom du mod√®le

## üîÑ Fonctionnement

### Flux de traitement

1. **G√©n√©ration d'embeddings** : Pour chaque batch de textes, le syst√®me :
   - Appelle l'API Ollama `/api/embeddings` avec le mod√®le `nomic-embed-text:v1.5`
   - G√©n√®re un vecteur d'embedding pour chaque texte
   - Stocke les embeddings en m√©moire

2. **Enrichissement du contexte** : Les embeddings sont utilis√©s pour :
   - Informer le LLM principal que les textes ont √©t√© pr√©-trait√©s s√©mantiquement
   - Am√©liorer la compr√©hension contextuelle du LLM

3. **Analyse LLM** : Le LLM principal analyse les textes avec le contexte enrichi

### Format de l'API Ollama

L'endpoint utilis√© est : `http://localhost:11434/api/embeddings`

**Requ√™te** :
```json
{
  "model": "nomic-embed-text:v1.5",
  "prompt": "Texte √† analyser"
}
```

**R√©ponse** :
```json
{
  "embedding": [0.123, -0.456, 0.789, ...]
}
```

## üöÄ Utilisation

Une fois configur√©, le syst√®me utilisera automatiquement le mod√®le d'embedding s'il est d√©tect√© dans la configuration. Aucune action suppl√©mentaire n'est n√©cessaire.

### Logs de d√©bogage

Vous verrez dans les logs :
```
Using embedding model: nomic-embed-text:v1.5
Generating embeddings for 50 texts...
Generated 50 embeddings
First embedding dimension: 768
Embeddings generated successfully, proceeding with LLM analysis...
```

## ‚ö†Ô∏è Notes importantes

1. **Performance** : La g√©n√©ration d'embeddings ajoute un temps de traitement suppl√©mentaire. Pour de gros volumes, consid√©rez d'augmenter le timeout.

2. **Fallback** : Si la g√©n√©ration d'embeddings √©choue, le syst√®me continue sans embeddings et traite les textes normalement.

3. **Mod√®les multiples** : Si plusieurs mod√®les d'embedding sont configur√©s, le premier trouv√© sera utilis√©.

4. **Optimisation** : Pour am√©liorer les performances, vous pouvez :
   - Traiter les embeddings en parall√®le (√† impl√©menter)
   - Mettre en cache les embeddings pour les textes identiques
   - Utiliser des batches plus petits pour les embeddings

## üîç D√©pannage

### Le mod√®le d'embedding n'est pas d√©tect√©

- V√©rifiez que le nom du mod√®le contient "embed" ou "nomic-embed"
- V√©rifiez que le mod√®le est bien configur√© dans les settings

### Erreur de connexion √† Ollama

- V√©rifiez que Ollama est d√©marr√© : `ollama serve`
- V√©rifiez l'URL dans la configuration (par d√©faut : `http://localhost:11434`)
- V√©rifiez que le mod√®le est t√©l√©charg√© : `ollama list`

### Timeout lors de la g√©n√©ration d'embeddings

- Augmentez le timeout dans le code (actuellement 60 secondes)
- R√©duisez la taille des batches
- V√©rifiez les performances de votre machine

## üìù Exemple de configuration compl√®te

```json
{
  "uid": "embedding-model-001",
  "data": {
    "model": "nomic-embed-text:v1.5",
    "baseUrl": "http://localhost:11434",
    "apiKey": "",
    "retryRequests": 3,
    "paginateRowsLimit": 500
  }
}
```

